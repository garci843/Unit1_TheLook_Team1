My Tasks:

My tasks for this project were to handle the entire streaming process of Earthquake Data from the USGS pipeline. I created the cloud function using main.py as well as requirements.txt. I then created a pub/sub topic and subscription that tied in directly with my cloud function, so that anytime the function was run, the data I requested was published into the topic and pulled using the subscription. Next, I created a cloud scheduler that runs the function at midnight EST every day (05:00 UTC). I then created a Bigquery table and developed the schema to match the JSON flattening function in my main.py function. Once I verified it was all set, I created a Dataflow job that sent information from the pub/sub into the BigQuery Table, and set it to expire in a month. I also included data from the last 30 days as a base, and data from the past ten years of massive earthquakes to address class imbalance for magnitude. The larger magnitude earthquake data was a separate streaming process so I repeated the process above, then appended the two streaming results into one table called earthquakes_cleaned. Building upon the established BigQuery table, I created a Python notebook focused on developing and evaluating a machine learning model to predict the occurrence of high-magnitude earthquakes in specific geographic regions for the upcoming month. This involved extensive feature engineering to create time-series indicators like lag features and time since the last event, addressing the severe class imbalance using SMOTE (Synthetic Minority Oversampling Technique), and training a LightGBM Classifier. Lastly, I created a Looker Studio Dashboard that gives an overview of the data I collected, as well as interactive features that allow you to study each earthquake type (magnitude bin).

Lessons Learned:
One of the main lessons I learned from this project was how to effectively build and reason about streaming data systems from end to end. Working with live earthquake data taught me how real-time ingestion differs from batch processing, especially when dealing with schema consistency, deduplication, and event-driven pipelines. I also learned how important it is to rely on established patterns—such as revisiting earlier labs and pipeline templates—when working through complex cloud infrastructure, rather than trying to reinvent solutions from scratch. Through this process, I gained hands-on experience with GCP services like Cloud Functions, Pub/Sub, Dataflow, and BigQuery, and developed a much stronger understanding of how streaming data flows through a modern analytics stack. This project significantly improved my confidence working with cloud-based data pipelines and real-time analytics.

